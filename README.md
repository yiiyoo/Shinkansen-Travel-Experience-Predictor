# Customer Purchase Prediction â€“ MIT Hackathon (Top 3)

This project was developed for the MIT IDSS Data Science Hackathon, where our team placed **3rd out of 25 teams**, achieving an accuracy of **0.9538** on the final model.  
We built an ensemble of gradient boosting models and tuned hyperparameters to predict customer purchase behaviors.

## **Project Overview**
- Loaded and explored customer purchase dataset to identify predictive features
- Applied extensive feature engineering and data preprocessing
- Built, tuned, and evaluated ensemble models (XGBoost, LightGBM, CatBoost)
- Optimized hyperparameters with Optuna's TPE sampler
- Interpreted model predictions using SHAP to identify feature importance
- Achieved **0.9538 accuracy**, outperforming most teams

## **Technologies & Libraries**
- Python (pandas, NumPy)
- scikit-learn
- XGBoost (`xgboost`)
- LightGBM (`lightgbm`)
- CatBoost (`catboost`)
- Optuna (hyperparameter tuning)
- SHAP (model explainability)
- Jupyter Notebook

## **Purpose**
- Showcase advanced supervised machine learning pipeline
- Demonstrate hyperparameter tuning and interpretability in a real-world scenario
- Portfolio-ready project illustrating collaborative data science and ML engineering

## **How to Run**
Clone this repository, install requirements, and open the notebook in Jupyter:

pip install -r requirements.txt
jupyter notebook Hackathon_to_submit.ipynb

## **Author**
Gabriel Ivan Ortega (Yiyo)
